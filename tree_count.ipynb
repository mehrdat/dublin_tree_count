{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\"/Users/Mehr/Documents/projects/tree/1.jpg\"\n",
    "import cv2 as cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, transform, img_as_ubyte\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def resize_image_skimage(input_dir, output_dir, target_size):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(input_dir, filename)\n",
    "            try:\n",
    "                image = io.imread(image_path)\n",
    "                resized_image = transform.resize(image, target_size, anti_aliasing=True)\n",
    "                \n",
    "                # Convert RGBA to RGB\n",
    "                if resized_image.shape[-1] == 4:\n",
    "                    resized_image = resized_image[..., :3]\n",
    "                \n",
    "                resized_image = img_as_ubyte(resized_image)\n",
    "                output_path = os.path.join(output_dir, os.path.splitext(filename)[0] + \".jpg\")\n",
    "                io.imsave(output_path, resized_image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "input_directory = '/Users/Mehr/Documents/projects/tree/'\n",
    "output_directory = '/Users/Mehr/Documents/projects/tree/resized/'\n",
    "target_size = (224, 224)  # Example target size\n",
    "\n",
    "#resize_image_skimage(input_directory, output_directory, target_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from skimage.feature import hog\n",
    "import os\n",
    "\n",
    "def extract(path):\n",
    "    image_path = os.path.join(\"/Users/Mehr/Documents/projects/tree/resized/\", path)\n",
    "    image = cv.imread(image_path)\n",
    "\n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image at {image_path}\")\n",
    "        return None\n",
    "\n",
    "    # Convert to grayscale\n",
    "    image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    features, hog_image = hog(image, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return features\n",
    "\n",
    "# Example usage\n",
    "path = \"1.jpg\"  # Replace with your actual image filename\n",
    "features = extract(path)\n",
    "if features is not None:\n",
    "    print(\"Features extracted successfully\")\n",
    "else:\n",
    "    print(\"Failed to extract features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "# Define the feature extraction function\n",
    "def extract_features(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    features, hog_image = hog(image, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return features\n",
    "\n",
    "# Example data\n",
    "# data = pd.DataFrame({\n",
    "#     'filenames': ['image1.jpg', 'image2.jpg'],  # Replace with your actual filenames\n",
    "#     'tree_count': [5, 3]  # Replace with your actual tree counts\n",
    "# })\n",
    "\n",
    "data=pd.read_csv(\"/Users/Mehr/Documents/projects/tree/resized/tree.csv\")\n",
    "data=data[:4]\n",
    "\n",
    "def extract(path):\n",
    "    image=cv.imread(\"/Users/Mehr/Documents/projects/tree/resized/\"+path)\n",
    "    image=cv.cvtColor(image,cv.COLOR_BGR2GRAY)\n",
    "    features, hog_image = hog(image, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=True,feature_vector=True)\n",
    "    return features\n",
    "\n",
    "# Extract features and check lengths\n",
    "data['features'] = data['filenames'].apply(extract)\n",
    "feature_lengths = data['features'].apply(len)\n",
    "\n",
    "# Ensure all feature vectors have the same length\n",
    "if len(feature_lengths.unique()) == 1:\n",
    "    X = np.array(data['features'].tolist())\n",
    "    y = data['tree_count'].values\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    model = RandomForestRegressor(n_estimators=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "else:\n",
    "    print(\"Inconsistent feature lengths detected:\", feature_lengths.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tree_count(image_path,model):\n",
    "    features=extract(image_path)\n",
    "    if features is None:\n",
    "            print(f\"Error: Could not extract features from {image_path}.\")\n",
    "            return None\n",
    "    features=np.array(features).reshape(1,-1)\n",
    "    prediction=model.predict(features)\n",
    "    return prediction[0]\n",
    "\n",
    "new_image_path = \"/Users/Mehr/Documents/projects/tree/test1/test.jpg\"\n",
    "    \n",
    "    # Predict the tree count in the new image\n",
    "predicted_count = predict_tree_count(new_image_path, model)\n",
    "print(f\"Predicted number of trees in the new image: {predicted_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import os\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Define the feature extraction function\n",
    "def extract_features(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image path {image_path} does not exist.\")\n",
    "        return None\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Failed to read image {image_path}.\")\n",
    "        return None\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    features, hog_image = hog(image, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return features\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"/Users/Mehr/Documents/projects/tree/resized/tree.csv\")\n",
    "#data = data[:14]\n",
    "\n",
    "# Extract features and check lengths\n",
    "data['features'] = data['filenames'].apply(lambda x: extract_features(\"/Users/Mehr/Documents/projects/tree/resized/\" + x))\n",
    "data = data.dropna(subset=['features'])  # Drop rows where features extraction failed\n",
    "feature_lengths = data['features'].apply(len)\n",
    "\n",
    "# Ensure all feature vectors have the same length\n",
    "if len(feature_lengths.unique()) == 1:\n",
    "    X = np.array(data['features'].tolist())\n",
    "    y = data['tree_count'].values\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model\n",
    "    #model = RandomForestRegressor(n_estimators=200,max_depth=20,min_samples_leaf=3,max_features='sqrt')\n",
    "    #model=xgb.XGBRegressor(n_estimators=100,max_depth=20,learning_rate=0.01)\n",
    "    #model=GradientBoostingRegressor(n_estimators=200,max_depth=20,min_samples_leaf=3,max_features='sqrt')\n",
    "    model=SVR(kernel='rbf',C=100,epsilon=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    \n",
    "    # Define a function to predict the number of trees in a new image\n",
    "    def predict_tree_count(image_path, model):\n",
    "        features = extract_features(image_path)\n",
    "        if features is None:\n",
    "            print(f\"Error: Could not extract features from {image_path}.\")\n",
    "            return None\n",
    "        features = np.array(features).reshape(1, -1)  # Reshape to 2D array\n",
    "        prediction = model.predict(features)\n",
    "        return prediction[0]\n",
    "    \n",
    "    # Path to the new image\n",
    "    new_image_path = \"/Users/Mehr/Documents/projects/tree/resized/test.jpg\"\n",
    "    \n",
    "    # Predict the tree count in the new image\n",
    "    predicted_count = predict_tree_count(new_image_path, model)\n",
    "    if predicted_count is not None:\n",
    "        print(f\"Predicted number of trees in the new image: {predicted_count}\")\n",
    "else:\n",
    "    print(\"Inconsistent feature lengths detected:\", feature_lengths.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define the feature extraction function\n",
    "def extract_features(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: Image path {image_path} does not exist.\")\n",
    "        return None\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Failed to read image {image_path}.\")\n",
    "        return None\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    features, hog_image = hog(image, pixels_per_cell=(16, 16), cells_per_block=(2, 2), visualize=True, feature_vector=True)\n",
    "    return features\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"/Users/Mehr/Documents/projects/tree/resized/tree.csv\")\n",
    "data = data[:4]\n",
    "\n",
    "# Extract features and check lengths\n",
    "data['features'] = data['filenames'].apply(lambda x: extract_features(\"/Users/Mehr/Documents/projects/tree/resized/\" + x))\n",
    "data = data.dropna(subset=['features'])  # Drop rows where features extraction failed\n",
    "feature_lengths = data['features'].apply(len)\n",
    "\n",
    "\n",
    "def best_model_params(models,X,y):    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    \n",
    "    mse=[]\n",
    "    predict_tree=[]\n",
    "    best_params=[]\n",
    "    model_name=[]\n",
    "    estimator=[]\n",
    "    for model in models:\n",
    "    # Define the parameter grid\n",
    "        if isinstance(model,RandomForestRegressor):\n",
    "            param_grid = {\n",
    "            'n_estimators': [30,50,100],\n",
    "            'max_depth': [2,3,5],\n",
    "            'min_samples_split': [2, 5,10],\n",
    "        }\n",
    "        elif isinstance(model,xgb.XGBRegressor):\n",
    "            param_grid = {\n",
    "            'n_estimators': [30, 50,100],\n",
    "            'max_depth': [2,3,5],\n",
    "            'min_samples_split': [2, 5,10],\n",
    "            'learning_rate': [.0001,.001,0.01],\n",
    "        }\n",
    "        elif isinstance(model,GradientBoostingRegressor):\n",
    "            param_grid = {\n",
    "            'n_estimators': [30, 50,100],\n",
    "            'max_depth': [2,3,5],\n",
    "            'min_samples_split': [2, 5,10],\n",
    "            'learning_rate': [.0001,.001,0.01],\n",
    "        }\n",
    "        elif isinstance(model,SVR):\n",
    "            param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'epsilon': [0.01, 0.1, 1],\n",
    "            'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "        \n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=1, verbose=1)  # Disable parallel processing\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        model_name.append(type(model).__name__)\n",
    "        estimator.append(grid_search.best_estimator_)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        predictions = best_model.predict(X_test)\n",
    "        \n",
    "        mse.append(mean_squared_error(y_test, predictions))\n",
    "        predict_tree.append(predictions)\n",
    "        best_params.append(best_model.get_params())\n",
    "        \n",
    "    return pd.DataFrame({'model_name':model_name,'estimator':estimator,'mse':mse,'predict_tree':predict_tree,'best_params':best_params})\n",
    "    \n",
    "\n",
    "# Ensure all feature vectors have the same length\n",
    "if len(feature_lengths.unique()) == 1:\n",
    "    X = np.array(data['features'].tolist())\n",
    "    y = data['tree_count'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #model = RandomForestRegressor()\n",
    "    #model=xgb.XGBRegressor()\n",
    "    #model=GradientBoostingRegressor()\n",
    "    #model=SVR()\n",
    "    models=[RandomForestRegressor(),xgb.XGBRegressor(),GradientBoostingRegressor(),SVR()]\n",
    "    best_models=best_model_params(models,X,y)\n",
    "    #best_model=xgb.XGBRegressor(min_samples_split=1, learning_rate=0.01,max_depth=3,n_estimators=50)\n",
    "    \n",
    "    print(best_models)\n",
    "    \n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    # #best_model.fit(X_train, y_train) \n",
    "    \n",
    "    # predictions = best_model.predict(X_test)\n",
    "    # mse = mean_squared_error(y_test, predictions)\n",
    "    # print(f\"Mean Squared Error: {mse}\")\n",
    "    \n",
    "    # Define a function to predict the number of trees in a new image\n",
    "    def predict_tree_count(image_path, models):\n",
    "        features = extract_features(image_path)\n",
    "        if features is None:\n",
    "            print(f\"Error: Could not extract features from {image_path}.\")\n",
    "            return None\n",
    "        features = np.array(features).reshape(1, -1)  # Reshape to 2D array\n",
    "        predictions=[]\n",
    "        model_names=[]\n",
    "        for model in models['estimator']:\n",
    "            predictions.append(model.predict(features))\n",
    "            model_names.append(type(model).__name__)   \n",
    "        return pd.DataFrame({'model_name':model_names,'predictions':predictions})\n",
    "    \n",
    "    # Path to the new image\n",
    "    new_image_path = \"/Users/Mehr/Documents/projects/tree/resized/test.jpg\"\n",
    "    \n",
    "    # Predict the tree count in the new image\n",
    "    predicted_count = predict_tree_count(new_image_path, best_models)\n",
    "    if predicted_count is not None:\n",
    "        #print(f\"Predicted number of trees in the new image: {predicted_count}\")\n",
    "        best_models['predictions']=predicted_count['predictions']\n",
    "        best_models\n",
    "else:\n",
    "    print(\"Inconsistent feature lengths detected:\", feature_lengths.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
